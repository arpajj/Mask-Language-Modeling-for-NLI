{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The template is: 0 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 1 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 2 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 3 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 4 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 5 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 6 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 7 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 8 and the Short and Long items are respectively: 5614 and 4210\n",
      "The template is: 9 and the Short and Long items are respectively: 5614 and 4210\n",
      "Finished with all DataLoaders creation!!!\n"
     ]
    }
   ],
   "source": [
    "from Helpers import make_dataloaders, main\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model_roberta = RobertaForMaskedLM.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BART model and tokenizer\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = make_dataloaders.labels_final\n",
    "valid_labels = all_labels[0]\n",
    "test_labels = all_labels[1]\n",
    "all_dataloaders = make_dataloaders.all_dataloaders\n",
    "all_short_dls = make_dataloaders.all_short_dataloaders\n",
    "all_long_dls = make_dataloaders.all_long_dataloaders\n",
    "all_valid_dataloaders = [x[0] for x in all_dataloaders]\n",
    "all_test_dataloaders = [x[1] for x in all_dataloaders]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_entail = ['miracles', 'poetic', 'innocent', 'contrary', 'open', 'ugly', 'air', 'documentary', 'from', 'faith', 'of', 'sea', 'ironic', 'animal', 'bird', 'amusing', 'itself', 'failed', 'above', 'supernatural', 'flight', 'grave', 'lit', 'knows', 'dumb', 'fire', 'here', 'wings', 'humorous', 'holy', 'ghost', 'laughter', 'lame', 'final', 'deep', 'qu', 'hear', 'murder', 'baby', 'rare', 'isn', 'invisible', 'dream', '？', 'moral', 'nature', 'misleading', 'terrible', 'gross', 'flying']\n",
    "tokens_neutral = ['history', 'smooth', 'people', 'incredible', 'consistent', 'cold', 'musical', 'typical', 'balanced', 'tails', 'dance', 'truths', 'alternative', 'stupid', 'knowledge', 'beat', 'unclear', 'double', 'historical', 'temporary', 'independent', 'quiet', 'literally', 'military', 'running', 'steady', 'louder', 'tide', 'crazy', 'asleep', 'english', 'sounds', 'female', 'hush', 'practice', 'ocean', 'girl', 'personal', '##¤', 'yep', 'calm', 'chinese']\n",
    "tokens_contra = ['speak', 'soccer', '/', 'odds', 'scoring', 'yourself', 'replay', 'wait', 'away', 'medical', 'version', 'fool', 'remarkable', 'match', 'loaded', 'reason', 'says', 'gun', 'score', 'patient', 'heart', 'goals', 'pretend', 'holds', 'football', 'wonderful', 'chances', 'string', 'expected', 'does', 'clever', 'goal', 'inside', 'repeated', 'scored', 'return', 'smart', 'human', 'lose', 'special', 'yield', 'baseball', 'has', 'can', 'try', 'paper', 'ready', 'successful', 'which', 'bet', 'police', 'probability', 'aye', 'exists', 'draw', 'whatever']\n",
    "\n",
    "tokens_of_interest = tokens_entail + tokens_contra + tokens_neutral\n",
    "three_lengths = [len(tokens_entail), len(tokens_neutral), len(tokens_contra)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_classical = [\"yes\", \"right\", \"true\", \"no\", \"wrong\", \"false\", \"?\", \"!\", \".\", \",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "On batch 0 out of 1228 bathces\n",
      "On batch 5 out of 1228 bathces\n",
      "On batch 10 out of 1228 bathces\n",
      "On batch 15 out of 1228 bathces\n",
      "Finished with the classical way\n",
      "\n",
      "On batch 0 out of 1228 bathces\n",
      "On batch 5 out of 1228 bathces\n",
      "On batch 10 out of 1228 bathces\n",
      "On batch 15 out of 1228 bathces\n",
      "Finished with the important tokens way\n",
      "Finished with the template 0\n"
     ]
    }
   ],
   "source": [
    "# Select a model and a tokenizer with a random pick, otherwise select intenionally\n",
    "index, model = random.choice(list(enumerate([model_bert, model_roberta])))\n",
    "tokenizer = [tokenizer_bert, tokenizer_roberta][index]\n",
    "# Select one of the 10 templates that we want to run (0 to 9)\n",
    "idx = 0\n",
    "probs_of_models_over_dateset = []\n",
    "top_tokens_per_model = []\n",
    "# remove the \"if statements\" inside the loop if you want to run all templates at once.\n",
    "for k, dl in enumerate(all_test_dataloaders):\n",
    "    if (k<idx):\n",
    "        continue    \n",
    "    print(model.name_or_path)\n",
    "    probs_over_dateset, _ = main.prob_distribution_over_vocab_with_batch(model, tokenizer, dl, tokens_classical, 100)\n",
    "    print(\"Finished with the classical way\")\n",
    "    print()\n",
    "    _, top_tokens_seen = main.prob_distribution_over_vocab_with_batch(model, tokenizer, dl, tokens_of_interest, 150)\n",
    "    probs_of_models_over_dateset.append(probs_over_dateset)\n",
    "    top_tokens_per_model.append(top_tokens_seen)\n",
    "    print(\"Finished with the important tokens way\")\n",
    "    print(\"Finished with the template {}\".format(k))\n",
    "    if(k==idx):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted labels of with the classical method are: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "The accuracy score of BERT is: 0.3309\n",
      "The predicted labels of with the new method are: [0, 0, 0, 2, 1, 2, 2, 2, 0, 0, 1, 1, 2, 0, 2, 2, 2, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 2, 0, 1, 2, 1, 0, 1, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 1, 1, 0, 2, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 1]\n",
      "The accuracy score the new method for BERT is: 0.3382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(probs_of_models_over_dateset)):\n",
    "    all_predicted_lbls = main.find_accuracy_imp_tokens(probs_of_models_over_dateset[i], top_tokens_per_model[i], test_labels, model, tokens_of_interest, three_lengths)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short & Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "## Here choose a template as 'idx' possible numbers: 0-9\n",
    "# Create the respective short and long dataloaders\n",
    "idx = 0\n",
    "my_dataloader = all_dataloaders[idx][1]\n",
    "my_short_dataloader = all_short_dls[idx]\n",
    "my_long_dataloader = all_long_dls[idx]\n",
    "both_dataloaders = [my_short_dataloader, my_long_dataloader]\n",
    "for s,l in zip(my_short_dataloader,my_long_dataloader):\n",
    "    print(len(s['sentence'][0].split()))\n",
    "    print(len(l['sentence'][0].split()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This script helped to find the average length of sentence in each tempalte\n",
    "#--------------------------------------------------------------------------\n",
    "# sum_lengths = np.zeros(len(all_dataloaders))\n",
    "# length = 0\n",
    "# short_sentences = []\n",
    "# long_sentences = []\n",
    "# for j, my_batch in enumerate(my_dataloader):\n",
    "#     for sent in my_batch['sentence']:\n",
    "#         # if(len(sent.split())<=27):\n",
    "#         #     short_sentences.append(sent)\n",
    "#         # else: \n",
    "#         #     long_sentences.append(sent)\n",
    "#         length = length + len(sent.split())\n",
    "    \n",
    "# print(length/(j*8))\n",
    "# print(len(short_sentences))\n",
    "# print(len(long_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "On batch 0 out of 702 bathces\n",
      "On batch 5 out of 702 bathces\n",
      "On batch 10 out of 702 bathces\n",
      "On batch 15 out of 702 bathces\n",
      "Finished model 1: bert \n",
      "\n",
      "roberta-base\n",
      "On batch 0 out of 702 bathces\n",
      "On batch 5 out of 702 bathces\n",
      "On batch 10 out of 702 bathces\n",
      "On batch 15 out of 702 bathces\n",
      "Finished model 2: roberta \n",
      "\n",
      "gpt2\n",
      "On batch 0 out of 702 bathces\n",
      "On batch 5 out of 702 bathces\n",
      "On batch 10 out of 702 bathces\n",
      "On batch 15 out of 702 bathces\n",
      "Finished model 3: gpt2 \n",
      "\n",
      "facebook/bart-base\n",
      "On batch 0 out of 702 bathces\n",
      "On batch 5 out of 702 bathces\n",
      "On batch 10 out of 702 bathces\n",
      "On batch 15 out of 702 bathces\n",
      "Finished model 4: facebook/bart \n",
      "\n",
      " Finished the Short DataLoaders:\n",
      "\n",
      "bert-base-uncased\n",
      "On batch 0 out of 527 bathces\n",
      "On batch 5 out of 527 bathces\n",
      "On batch 10 out of 527 bathces\n",
      "On batch 15 out of 527 bathces\n",
      "Finished model 1: bert \n",
      "\n",
      "roberta-base\n",
      "On batch 0 out of 527 bathces\n",
      "On batch 5 out of 527 bathces\n",
      "On batch 10 out of 527 bathces\n",
      "On batch 15 out of 527 bathces\n",
      "Finished model 2: roberta \n",
      "\n",
      "gpt2\n",
      "On batch 0 out of 527 bathces\n",
      "On batch 5 out of 527 bathces\n",
      "On batch 10 out of 527 bathces\n",
      "On batch 15 out of 527 bathces\n",
      "Finished model 3: gpt2 \n",
      "\n",
      "facebook/bart-base\n",
      "On batch 0 out of 527 bathces\n",
      "On batch 5 out of 527 bathces\n",
      "On batch 10 out of 527 bathces\n",
      "On batch 15 out of 527 bathces\n",
      "Finished model 4: facebook/bart \n",
      "\n",
      " Finished the Long DataLoaders:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that we use the the important tokens but they are ... unimportant in this method\n",
    "models = [model_bert, model_roberta, model_gpt2, model_bart]\n",
    "tokenizers = [tokenizer_bert, tokenizer_roberta, tokenizer_gpt2, tokenizer_bart]\n",
    "probs_of_models_over_dateset = []\n",
    "top_tokens_per_model = []\n",
    "for k, dl in enumerate(both_dataloaders):  \n",
    "    for i, (mdl,tokzer) in enumerate(zip(models,tokenizers)):\n",
    "        print(mdl.name_or_path)\n",
    "        probs_over_dateset, top_tokens_seen = main.prob_distribution_over_vocab_with_batch(mdl, tokzer, dl, tokens_classical, 100)\n",
    "        probs_of_models_over_dateset.append(probs_over_dateset)\n",
    "        top_tokens_per_model.append(top_tokens_seen)\n",
    "        print(\"Finished model {}: {} \\n\".format(i+1, mdl.name_or_path.split(\"-\")[0]))\n",
    "    if k==0:\n",
    "        print(\" Finished the {} DataLoaders:\".format(\"Short\"))\n",
    "        print()\n",
    "    if k==1: \n",
    "        print(\" Finished the {} DataLoaders:\".format(\"Long\"))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_probs_of_models = probs_of_models_over_dateset[0:4]\n",
    "long_probs_of_models = probs_of_models_over_dateset[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted labels of BERT are: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "The accuracy score of BERT is: 0.3456\n",
      "\n",
      "The predicted labels of RoBERTa are: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "The accuracy score of RoBERTa is: 0.3015\n",
      "\n",
      "The predicted labels of GPT2 are: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "The accuracy score of GPT2 is: 0.3603\n",
      "\n",
      "The predicted labels of BART are: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "The accuracy score of BART is: 0.3015\n",
      "\n",
      "The accuracy score the random method is: 0.2868\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The predicted labels of BERT are: [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "The accuracy score of BERT is: 0.3235\n",
      "\n",
      "The predicted labels of RoBERTa are: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "The accuracy score of RoBERTa is: 0.3015\n",
      "\n",
      "The predicted labels of GPT2 are: [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "The accuracy score of GPT2 is: 0.3603\n",
      "\n",
      "The predicted labels of BART are: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "The accuracy score of BART is: 0.3015\n",
      "\n",
      "The accuracy score the random method is: 0.3824\n"
     ]
    }
   ],
   "source": [
    "all_predicted_lbls = main.find_accuracy_short_long(short_probs_of_models, test_labels)\n",
    "print()\n",
    "print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "all_predicted_lbls = main.find_accuracy_short_long(long_probs_of_models, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
